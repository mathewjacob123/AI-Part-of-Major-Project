{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import json\n",
    "from typing import List, Dict\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class LLaMA2HomeAutomation:\n",
    "    def __init__(self, model_size: str = \"7B\"):\n",
    "        \"\"\"Initialize LLaMA 2 for home automation\"\"\"\n",
    "        bnb_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    "            bnb_4bit_compute_dtype=torch.float16,\n",
    "            bnb_4bit_use_double_quant=True\n",
    "        )\n",
    "\n",
    "        model_name = f\"meta-llama/Llama-2-{model_size}-chat-hf\"\n",
    "\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            model_name,\n",
    "            token=\"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\"\n",
    "        )\n",
    "\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            quantization_config=bnb_config,\n",
    "            device_map=\"auto\",\n",
    "            token=\"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\"\n",
    "        )\n",
    "\n",
    "    def _create_system_prompt(self) -> str:\n",
    "        \"\"\"Create system prompt for LLaMA 2.\"\"\"\n",
    "        return \"\"\"You are a home automation AI assistant specialized in analyzing device usage patterns and creating automation schedules.\n",
    "        You must respond ONLY with valid JSON that follows this exact format:\n",
    "\n",
    "        {\n",
    "          \"list\": [\n",
    "            [\"ONF\", \"device_id\", \"state\", \"schedule_id\"],\n",
    "            [\"ONF\", \"device_id\", \"state\", \"schedule_id\"]\n",
    "          ],\n",
    "          \"id\": \"schedule_id\",\n",
    "          \"time\": \"HHMMSS\"\n",
    "        }\n",
    "\n",
    "        Important format rules:\n",
    "        1. State format for 2LF devices:\n",
    "           - First character for light: 0=off, 1=on, N=neglect\n",
    "           - Second character for fan: 0-4 for speed, N=neglect\n",
    "           - \"NN\" means neglect both\n",
    "        2. Time must be in 24-hour format with 6 digits (HHMMSS)\n",
    "        3. Schedule ID must be unique and start with \"ID\" followed by timestamp\n",
    "        4. All devices use \"ONF\" as the function type\n",
    "        5. Each schedule must contain exactly two list items for the same device\n",
    "\n",
    "        Do not include any explanations or additional text in your response. Output only the JSON schedule.\"\"\"\n",
    "\n",
    "    def _create_user_prompt(self, history_data: List[Dict]) -> str:\n",
    "        \"\"\"Create detailed prompt from history data.\"\"\"\n",
    "        prompt = \"Based on this device usage history:\\n\\n\"\n",
    "\n",
    "        # Group and sort events by date\n",
    "        events_by_date = {}\n",
    "        for event in sorted(history_data, key=lambda x: (x['date'], x['time'])):\n",
    "            date = event['date']\n",
    "            if date not in events_by_date:\n",
    "                events_by_date[date] = []\n",
    "            events_by_date[date].append(event)\n",
    "\n",
    "        # Format events into readable text\n",
    "        for date, events in events_by_date.items():\n",
    "            prompt += f\"Date: {date}\\n\"\n",
    "            for event in events:\n",
    "                prompt += (f\"Time: {event['time']}, Room: {event['room_id']}, \"\n",
    "                          f\"Device: {event['switchbox_id']}, State: {event['state']}\\n\")\n",
    "            prompt += \"\\n\"\n",
    "\n",
    "        prompt += \"\"\"Create a schedule for when you see a clear pattern in the data.\n",
    "        The schedule must follow these rules:\n",
    "        1. Each schedule must contain two actions for the same device\n",
    "        2. First action should use \"NN\" state to keep current light state\n",
    "        3. Second action should use \"NX\" state where X is the desired fan speed (0-4)\n",
    "        4. Time must match when the pattern consistently occurs\n",
    "        5. Generate a unique schedule ID starting with \"ID\" followed by current timestamp\n",
    "\n",
    "        Respond only with the JSON schedule. Do not include any other text.\"\"\"\n",
    "\n",
    "        return prompt\n",
    "\n",
    "    def _format_chat_message(self, system_prompt: str, user_prompt: str) -> str:\n",
    "        \"\"\"Format prompts in LLaMA 2 chat format.\"\"\"\n",
    "        return f\"\"\"<s>[INST] <<SYS>>\n",
    "{system_prompt}\n",
    "<</SYS>>\n",
    "\n",
    "{user_prompt} [/INST]\"\"\"\n",
    "\n",
    "    def _generate_schedule_id(self) -> str:\n",
    "        \"\"\"Generate unique schedule ID.\"\"\"\n",
    "        return f\"ID{datetime.now().strftime('%Y%m%d%H%M%S')}\"\n",
    "\n",
    "    def _parse_llm_response(self, response: str) -> Dict:\n",
    "        \"\"\"Parse LLaMA 2 response to extract schedule.\"\"\"\n",
    "        try:\n",
    "            # Find the JSON content\n",
    "            start_idx = response.find('{')\n",
    "            end_idx = response.rfind('}') + 1\n",
    "            if start_idx >= 0 and end_idx > start_idx:\n",
    "                json_str = response[start_idx:end_idx]\n",
    "                schedule = json.loads(json_str)\n",
    "                return schedule\n",
    "            else:\n",
    "                print(\"No JSON found in response\")\n",
    "                return self._create_fallback_schedule()\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Failed to parse JSON: {str(e)}\")\n",
    "            return self._create_fallback_schedule()\n",
    "\n",
    "    def _create_fallback_schedule(self) -> Dict:\n",
    "        \"\"\"Create a basic fallback schedule if parsing fails.\"\"\"\n",
    "        schedule_id = self._generate_schedule_id()\n",
    "        return {\n",
    "            \"list\": [\n",
    "                [\"ONF\", \"2LF25092023114529\", \"NN\", schedule_id],\n",
    "                [\"ONF\", \"2LF25092023114529\", \"N0\", schedule_id]\n",
    "            ],\n",
    "            \"id\": schedule_id,\n",
    "            \"time\": \"070000\"  # Default to 7 AM\n",
    "        }\n",
    "\n",
    "    def generate_schedule(self, history_data: List[Dict]) -> Dict:\n",
    "        \"\"\"Generate automation schedule based on historical data.\"\"\"\n",
    "        try:\n",
    "            # Create prompts\n",
    "            system_prompt = self._create_system_prompt()\n",
    "            user_prompt = self._create_user_prompt(history_data)\n",
    "            chat_message = self._format_chat_message(system_prompt, user_prompt)\n",
    "\n",
    "            # Generate response using LLaMA 2\n",
    "            inputs = self.tokenizer(chat_message, return_tensors=\"pt\").to(self.model.device)\n",
    "            outputs = self.model.generate(\n",
    "                inputs.input_ids,\n",
    "                max_length=1024,\n",
    "                temperature=0.3,  # Lower temperature for more consistent output\n",
    "                do_sample=True,\n",
    "                top_p=0.95,\n",
    "                repetition_penalty=1.15\n",
    "            )\n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "            # Parse and return the schedule\n",
    "            return self._parse_llm_response(response)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating schedule: {str(e)}\")\n",
    "            return self._create_fallback_schedule()\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Sample history data\n",
    "    history_data = [\n",
    "        {\n",
    "            \"room_id\": \"Living Room\",\n",
    "            \"switchbox_id\": \"2LF25092023114529\",\n",
    "            \"state\": \"02\",\n",
    "            \"date\": \"2024-10-27\",\n",
    "            \"time\": \"210000\"\n",
    "        },\n",
    "        {\n",
    "            \"room_id\": \"Living Room\",\n",
    "            \"switchbox_id\": \"2LF25092023114529\",\n",
    "            \"state\": \"00\",\n",
    "            \"date\": \"2024-10-28\",\n",
    "            \"time\": \"070000\"\n",
    "        },\n",
    "        {\n",
    "            \"room_id\": \"Living Room\",\n",
    "            \"switchbox_id\": \"2LF25092023114529\",\n",
    "            \"state\": \"00\",\n",
    "            \"date\": \"2024-10-29\",\n",
    "            \"time\": \"070000\"\n",
    "        }\n",
    "    ]\n",
    "\n",
    "    # Initialize automation system\n",
    "    automation = LLaMA2HomeAutomation(model_size=\"7B\")\n",
    "\n",
    "    # Generate schedule\n",
    "    schedule = automation.generate_schedule(history_data)\n",
    "\n",
    "    # Print result\n",
    "    print(\"\\nGenerated Schedule:\")\n",
    "    print(json.dumps(schedule, indent=2))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
